{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "protective-railway",
   "metadata": {},
   "source": [
    "# 1. Identifying assumptions for regression\n",
    "\n",
    "For each of the questions in this section provide a short answer and argument. Note the quality and concision of the argument matters much more than the answer!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superb-force",
   "metadata": {},
   "source": [
    "**(1)** Evaluate the truth of following statement: ''In the linear regression $y = X\\beta + u$ the usual identifying assumption $\\mathbb{E}(u|X) = 0$ implies $E(h(X)u) = 0$ for any function $h$ satisfying some regularity conditions related to measurability.''\n",
    "\n",
    "**Answer**\n",
    "For functions satisfying regularity conditions, this statement is true. Because $X$ provides no information about the expected value of $u$, no function of $X$ is able to either. Formally,\n",
    "\n",
    "$\\mathbb{E}(u|X) = 0 \\Rightarrow \\mathbb{E}(u|h(X)) = 0 \\Rightarrow \\mathbb{E}(h(X)u) = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "orange-instrumentation",
   "metadata": {},
   "source": [
    "**(2)** Consider the same linear regression $y = X\\beta + u$, but now suppose an alternative identifying assumption $\\mathbb{E}(X|u) = 0$. Construct a simple estimator based on this alternative. Compare the usual and alternative identifying assumptions; are they equivalent? Is one stronger than the other?\n",
    "\n",
    "**Answer** A simple moment estimator given by this condition is derived by:\n",
    "\n",
    "$\\begin{align*}\n",
    "\\mathbb{E}(X|u) &= 0 \\Rightarrow \\mathbb{E}(u'X) = 0 \\Rightarrow \\mathbb{E}((y - X\\beta)'X) = 0 \\\\\n",
    "\\mathbb{E}(y'X) &= \\beta'\\mathbb{E}(X'X)\n",
    "\\end{align*}$\n",
    "\n",
    "which can be solved using the sample analog. \n",
    "\n",
    "The usual identifying assumption is $\\mathbb{E}(u|X) = 0$; here, the condition goes the other direction, $\\mathbb{E}(X|u) = 0$. While mathematically one is not stronger than the other in the sense that one does not imply the other, from a data generating point of view the alternative identifying assumption is more restrictive. Consider a regression model with an intercept, for example. If the disturbances had a non-zero mean, we could ''demean'' the disturbances by subtracting the mean and adding it to the intercept. Then, $\\mathbb{E}(u|X)$ is not restrictive (See Greene chapter 2 for a discussion of this). \n",
    "\n",
    "However, if we believe that $X$ comes from some real data-generating process, then $\\mathbb{E}(X|u) = 0 \\Rightarrow \\mathbb{E}(X) = 0$ is restrictive, because the columns of $X$ may each come from a different data generating process, and ''demeaning'' $X$ is not straightforward as before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "touched-dispute",
   "metadata": {},
   "source": [
    "**(3)** Suppose that $y = f(X) + u$ for some unknown but continuous function $f$. Suppose we want to use observed data on $X$ to predict outcomes $y$, and seek a predictor $\\hat{y}(X)$ which is ''best'' in the sense that the mean squared prediction error $\\mathbb{E}(y−\\hat{y}(X)|X))^2$ is minimized. What can we say about $\\hat{y}$ and its relation to the conditional expectation $E(y|X)$? Its relation to $u$?\n",
    "\n",
    "**Answer** First note that $\\hat{y}(X) = \\mathbb{E}(y|X)$ is the minimizer of this objective function:\n",
    "\n",
    "$\\text{argmin}_{\\hat{y}(X)} \\mathbb{E}(y−\\hat{y}(X)|X))^2 = \\mathbb{E}(y|X)$\n",
    "\n",
    "This implies that:\n",
    "\n",
    "$\\mathbb{E}(\\hat{y}(X)|X) = \\mathbb{E}(y|X) = \\mathbb{E}(f(X)|X) + \\mathbb{E}(u|X) = f(X) + \\mathbb{E}(u|X)$\n",
    "\n",
    "Thus under the identifying assumption $\\mathbb{E}(u|X) = 0$, $\\hat{y}(X)$ unbiasedly estimates $f(X)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organic-stress",
   "metadata": {},
   "source": [
    "# 2. Omitted variables\n",
    "\n",
    "You are asked to serve as a referee for a paper submitted to a top field journal. In the submitted paper the researcher uses a sample of size $N$ to estimate a model \n",
    "\n",
    "$y = \\alpha + \\beta x + u$\n",
    "\n",
    "The coefficient $\\beta$ seems to be significantly different from zero, but the researcher is concerned about omitted variable bias, so they also estimate a variety of alternative specifications of the form\n",
    "\n",
    "$y = \\alpha + \\beta x + \\gamma w + u$\n",
    "\n",
    "where $w$ is one of a number of other variables that the researcher hypothesizes might have some effect on y as a way of testing the first model.\n",
    "\n",
    "The researcher finds a particular variable $w$ which enters the regression significantly, and so \n",
    "\n",
    "(i) rejects the first model, concluding that the first estimate of $\\beta$ was in fact affected by omitted variable bias; \n",
    "\n",
    "(ii) declares the augmented regression to be their ''preferred specification''; and \n",
    "\n",
    "(iii) proceeds to construct standard t-statistics for $\\beta$ and $\\gamma$ as a way of proceeding with inference.\n",
    "\n",
    "Peer reviews in economics usually include some ''notes for the author.'' What might your notes say about the paper's approach to omitted variable bias? Comment specifically on each of (i), (ii), and (iii). Try to make your remarks critical yet constructive— what shortcomings do you see, and how might the author address these?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "front-inspection",
   "metadata": {},
   "source": [
    "**Answer** \n",
    "\n",
    "First I comment on their procedure for including extra control variables $w$ one by one. If the researchers have reasons to believe that a vector of relevant variables $W$ may be omitted, then their hypothesis is that $\\mathbb{E}(u|X\\cup W) = 0$, not $\\mathbb{E}(u|X\\cup w) = 0, w\\in W$. Therefore I would suggest that they also run a specification where they include the entire vector $W$. Now, for their procedure:\n",
    "\n",
    "(i) They reject the model based on the significance of the parameter associated with $w$. However, the fact that $w$ has significant covariance with $y$ does not imply that $X$ is endogeneous and consequently that $\\beta$ is biased. For example, if $w$ is correlated with $Y$ but orthogonal to $X$, then by the Frisch–Waugh–Lovell theorem, we know that the estimate of $b$ will be unaffected.  \n",
    "\n",
    "I am also concerned that there may be other specifications in which some $w'$ does not enter significantly, but whose presence significantly alters the estimate of $b$. Even though $\\gamma$ in this case would not be signficant, the fact that $b$ changes dramatically could be a sign of OVB. \n",
    "\n",
    "(ii) Based on the above, I believe the author's criteria for choosing their preferred specification is flawed. They should choose their ''preferred specification'' based on the behavior of $\\hat{\\beta}$, and not on the significant of $w$ (or a vector $W$). If the estimate of $\\hat{\\beta}$ is fairly consistent across specifications, then this choice may be based on theory or choosing an estimate that represents a middle ground or conservative estimate of the effect they are trying to measure. If the inclusion of a set of extra controls causes the estimate to change (regardless of the significance of $W$), this should probably be their preferred specification because it is the estimate that takes into account the possible confounding factors. \n",
    "\n",
    "If the different $\\hat{\\beta}$'s vary widely, they may have a serious OVB problem, and their identification model may need to be rethought.\n",
    "\n",
    "(iii) Once a preferred specification is chosen, the authors may construct t-statistics for any of the coefficients in the regression, though because it seems that the authors are primarily interested in conducting inference on $\\beta$, these statistics may be irrelevant for $\\gamma$. I think a more informative way to report their results would be to report the point estimate and t statistics for $\\beta$ from various specifications, so that the reader can judge if the value and significance of $\\beta$ appears stable when including additional controls. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "catholic-print",
   "metadata": {},
   "source": [
    "# 3. Breusch-Pagan Extended\n",
    "Consider a linear regression of the form\n",
    "\n",
    "$y = \\alpha + \\beta x + u$\n",
    "\n",
    "with $(y, x)$ both scalar random variables, where it is assumed that \n",
    "\n",
    "(a.i) $\\mathbb{E}(ux) = \\mathbb{E}(u) = 0$\n",
    "\n",
    "(a.ii) $\\mathbb{E}(u^2|x) = \\sigma^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "residential-crack",
   "metadata": {},
   "source": [
    "**(1)** The condition (a.i) is essentially untestable, but Breusch and Pagan (1979) argue that one can test (a.ii) via an auxiliary regression $\\hat{u}^2 = c + dx + e$, where the $\\hat{u}^2$ are the residuals from the first regression, and the test of (a.ii) then becomes a test of $H_0: d = 0$. Explain both why (a.i) is untestable, and the logic of the test of (a.ii.)\n",
    "\n",
    "**Answer**\n",
    "\n",
    "(a.i) is untestable because it is an assumption in the construction of OLS. Computationally, we can see the untestability of this assumption with the following. Suppose we wanted to test (a.i), so we construct a sample analog for this object.\n",
    "\n",
    "Let $X  = [\\mathbb{1}_N\\: x]$.\n",
    "\n",
    "$X'e = X'(y-Xb) = X'y - X'Xb = X'y - X'X(X'X)^{-1}X'y = X'y - X'y = 0$\n",
    "\n",
    "This object is zero always, so we cannot test the condition (a.i) in any way. \n",
    "\n",
    "The logic of (a.ii) follows from the fact that conditional homoskedasticity intuitively states that $u$ does not vary with $x$ in any way. Therefore, we should not be able to predict the value of $u^2$ with $x$ in any systematic way, which would imply $d = 0$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assumed-battlefield",
   "metadata": {},
   "source": [
    "**(2)** Use the two conditions (a.i) and (a.ii) to construct a GMM version of the Breusch-Pagan test.\n",
    "\n",
    "**Answer** My answer draws from Shreya Sarkar's formulation of a test for homoskedasticity on piazza ([@33_f2](https://piazza.com/class/km9z4xiuham24j?cid=33_f2)). (a.ii) can be transformed as follows:\n",
    "\n",
    "$\\mathbb{E}(u^2|X) = \\sigma^2 \\Rightarrow \\mathbb{E}(u^2 - \\sigma^2 | X) = 0 \\Rightarrow \\mathbb{E}(X'(u^2 - sigma^2)) = 0 \\Rightarrow \\mathbb{E}(X'(y - X'\\tilde{\\beta})^2 - X'\\sigma^2) = 0$\n",
    "\n",
    "where $\\tilde{\\beta} = [\\alpha\\; \\beta]'$. Stacking this condition with (a.i) gives a vector of 4 moment conditions:\n",
    "\n",
    "$\\mathbb{E}g_i(\\tilde{\\beta}, \\sigma^2) = \\mathbb{E}\\begin{bmatrix}\n",
    "X'_i(y_i - X'_i\\tilde{\\beta}) \\\\ \n",
    "X'_i(y_i - X'_i\\tilde{\\beta})^2 - X'_i\\sigma^2\n",
    "\\end{bmatrix} = \\mathbf{0}_4$\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$g_N(\\tilde{\\beta}, \\sigma^2) = \\begin{bmatrix}\n",
    "\\frac{1}{n}\\sum_i^N X'_i(y_i - X'_i\\tilde{\\beta}) \\\\ \n",
    "\\frac{1}{n}\\sum_i^NX'_i(y_i - X'_i\\tilde{\\beta})^2 - X'_i\\sigma^2\n",
    "\\end{bmatrix}$\n",
    "\n",
    "and the objective function to minimize by GMM with a generic weighting matrix $W$ is given by:\n",
    "\n",
    "$J(\\tilde{\\beta}, \\sigma^2) = Ng_N(\\tilde{\\beta}, \\sigma^2)'Wg_N(\\tilde{\\beta}, \\sigma^2)$\n",
    "\n",
    "So that the GMM test for homoskedasticity would be based on the statistic $J(\\tilde{\\beta}_{GMM}, \\sigma^2_{GMM})\\sim \\chi^2_{1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinct-royalty",
   "metadata": {},
   "source": [
    "**(3)** What can you say about the performance or relative merits of the Bruesch-Pagan test versus your GMM alternative?\n",
    "\n",
    "**Answer**\n",
    "One downside to the GMM test for homoskedasticity is that the test uses two distinct types of moments: moments related to the exogeneity of $x$, and moments related to homoskedasticity. Therefore, if the null hypothesis is rejected, we may not know if it was because of homoskedasticity or because of the exogeneity condition. However, the GMM test is more flexible than the Breusch-Pagan test, which assumes normality of the errors and, in this formulation, that any heterskedasticity is linear in $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contained-robin",
   "metadata": {},
   "source": [
    "**(4)** Suppose that in fact that $x$ is distributed uniformly over the interval $[0,2\\pi]$, and $E(u^2|x) = \\sigma^2(x) = \\sigma^2\\sin(x)$, thus violating (a.ii). What can you say about the performance of the Breusch-Pagan test in this circumstance? Can you modify your GMM test to provide a superior alternative?\n",
    "\n",
    "**Answer** \n",
    "\n",
    "The Breusch-Pagan test will not perform will in this circumstance, and will be prone to accepting the null hypothesis of homoskedasticity when it should be rejected. This is because $\\sin(x)$ cycles, so the net effect of $x$ on $\\hat{u}^2$ will tend to give an estimate of $b=0$. \n",
    "\n",
    "If we suspected that the variance of the disturbances took that form, we could directly incorporate that information into the moment conditions, replacing our homoskedasticity moments with:\n",
    "\n",
    "$\\mathbb{E}(X_i'u_i^2 - X_i'\\sin(X_i)\\sigma^2)$\n",
    "\n",
    "If we do not suspect the form of $\\sigma^2(x)$, but we think it may be mean zero and therefore want to modify the GMM approach to testing for homoskedasticity, we could include higher moments of the variance in the test. Under homoskedasticity, the variance of $\\sigma^2$ should be zero. However, under a heteroskedastic form, the variance will be positive. Therefore we could add the following to our moment conditions:\n",
    "\n",
    "$\\mathbb{E}(X_i'^2(u_i - \\sigma^2)^2) = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-contribution",
   "metadata": {},
   "source": [
    "**(5)** In the above, we’ve considered a test of a specific functional form for the variance of $u$. Suppose instead that we don’t have any prior information regarding the form of\n",
    "$\\mathbb{E}(u^2|x) = f(x)$. Discuss how you might go about constructing an extended version\n",
    "of the Breusch-Pagan test which tests for $f(x)$ non-constant.\n",
    "\n",
    "**Answer** \n",
    "\n",
    "In a similar vein to the previous proposal, if we suspect that $\\mathbb{E}(u^2|x) = f(x)$, we may want to approximate this non-linear relationship with basis functions. For example, on the interval $[0, 2\\pi]$, the function $\\sigma^2\\sin(x)$ may be reasonably approximated by a cubic basis function. Therefore we could modify the Breusch-Pagan test to be:\n",
    "\n",
    "$\\hat{u}^2 = c + d_1x + d_2x^2 + d_3x^3 + e$\n",
    "\n",
    "with the joint hypothesis \n",
    "\n",
    "$H_0: \\begin{bmatrix}\n",
    "d_1 \\\\\n",
    "d_2 \\\\ \n",
    "d_3\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "0 \\\\\n",
    "0\\\\ \n",
    "0\n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "royal-cornwall",
   "metadata": {},
   "source": [
    "**(6)** Show that you can use your ideas about estimating $f(x)$ to construct a more efficient\n",
    "estimator of $\\beta$ if $f(x)$ isn’t constant. Relate your estimator to the optimal generalized least squares (GLS) estimator.\n",
    "\n",
    "## FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decimal-pledge",
   "metadata": {},
   "source": [
    "# 4. Black Lives Matter\n",
    "Fryer Jr (2019) uses data on encounters between police and civilians of different races in the US to explore how police use of force is related to a civilian’s race. While Fryer finds that Black and Hispanic civilians are much more likely to ''experience some form of force'' from the police and while the probability of being shot by the police is much higher for a civilian who is Black or Hispanic, Fryer’s most prominent result is that for ''the most extreme use of force— officer-involved shootings— we find no racial differences either in the raw data or when contextual factors are taken into account.''\n",
    "\n",
    "Introducing some notation, let $R$ denote the civilian’s race; $U$ some variables observed by the police officer prior to any interaction (e.g., observing ''suspicious'' behavior) but not the econometrician; $D$ a binary variable indicating the event ($D = 1$) of an encounter between a given civilian and a police officer; $V$ a set of ''contextual factors'' related to the encounter and reported by the officer; and $S$ the event that the civilian is shot by the officer. We can then express Fry's finding regarding shootings as not being able to reject either\n",
    "\n",
    "(1) Pr(S|D = 1, R) = Pr(S|D = 1)\n",
    "\n",
    "or\n",
    "\n",
    "(2) Pr(S|D = 1,V,R) = Pr(S|D = 1,V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constitutional-alberta",
   "metadata": {},
   "source": [
    "**(1)** Durlauf and Heckman (2020) criticize this conclusion of Fryer's, on the grounds that $D$ may be an endogenous variable. You needn't read their paper, but explain in your own words what sorts of endogeneity might undermine Fryer's conclusion that the probability of being shot by the police doesn't depend on race.\n",
    "\n",
    "**Answer** \n",
    "\n",
    "If officers racially profile civilians, whether because they are personally prejudiced, unconsciously biased, or are following official guidelines that more heavily police minority neighborhoods, then they may interact more with innocent black civilians. That means that racial profiling is positively correlated with the probability of having an interaction with a police officer. If innocent people are more likely to comply with police officers (for example, less likely to flee from officers) then racial profiling may be negatively correlated with the probability of being shot by an officer. i.e., racial profiling is clearly positively correlated with $P(D | R = black)$ and negatively correlated with $S$. This omitted variable therefore would bias the estimate of the marginal effect of $R$ on $S$ downwards  downwards, which would throw doubt on our ability to estimate (1).\n",
    "\n",
    "A related but distinct source of endogeneity is officer bias given that an interaction has already ocurred (the previous was focused on bias that increases $P(D|R)$). If some police officers are prejudiced against black people and believe (consciously or inconsciously) that they are more dangerous than other ethnic groups, then this bias could be correlated with an increased propensity to exaggerate the variables $V$ experienced by the officer during the interaction, as well as an increased propensity to use deadly force given a perceived level of $V$. This source of endogeneity decreases our confidencein being able to test (2), because $V$ is reported by the officer, so it may not be accurate and may be correlated with their propensity to use force. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nominated-atlantic",
   "metadata": {},
   "source": [
    "**(2)** Spell out conditions on $(R,S,U,V,D,Z)$ (perhaps using a causal diagram) which would suffice to interpret (1) and (2) as evidence that there are no racial differences in the victims of police shootings. In particular, what does one need to assume about $Pr(D = 1|R,U)$?\n",
    "\n",
    "**Answer**\n",
    "The below causal diagram draws the relationships between $(R, S, U, D, V)$. The omitted variable of racial profiling described above is what causes the relationship between race and factors previous to the encounter or directly the probability of being stopped marked by (1). The omitted variable of bias that ocurs during the encounter is marked by the path from R to V, marked with (2). All of these signalled arrows must \"not exist\" (i.e., there is no relationship between race and getting stopped or the officer's perception of the danger level of the encounter) in order for equations (1) and (2) to be interpreted as evidence of no racial differences in the propensity to use force, because those equations attempt to only measure the direct line from $R$ to $S$. \n",
    "\n",
    "Note that equation (1) above attempts to do inference on $Pr(S|D=1,R,U)$. However, \n",
    "\n",
    "$Pr(S|D=1,R,U) = \\frac{Pr(S\\cap D = 1|R, U)}{P(D=1|R, U)}$\n",
    "\n",
    "but $P(D=1|R, U)$ is not observable just from police incidence reports. Therefore, we need the condition $Pr(D = 1|R,U) = Pr(D = 1 | U)$ (the probability of being stopped is independent of race) so that prejudice does not confound estimation by interfering in the denominator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "active-physics",
   "metadata": {},
   "source": [
    "![Causal diagram](causal_diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civic-discharge",
   "metadata": {},
   "source": [
    "**(3)** Consider an alternative (\"driving while Black\") model in which the police use race as a criterion for stopping or otherwise interacting with a given civilian. Compare the causal structure of this model with your answer to (1). Viewed through the lens of this model, how would one interpret Fry's failure to reject $Pr(S|D = 1, R) = Pr(S|D = 1)$?\n",
    "\n",
    "**Answer** The \"driving while Black\" model explicitly posits what I call the \"racial profiling\" confounding factor above. If police use race as a criteran for stopping a civilian, then $R$ directly influences $D$ in the causal diagram, so our \"exclusion restriction\" is violated. Fry fails to reject:\n",
    "\n",
    "$\\begin{align*}\n",
    "Pr(S|D = 1, R) &= Pr(S|D = 1) \\\\ \n",
    "\\frac{Pr(S\\cap D = 1|R)}{Pr(D=1|R)} &= \\frac{Pr(S\\cap D = 1)}{Pr(D=1)}\n",
    "\\end{align*}$\n",
    "\n",
    "If we suspect that the driving while Black model has relevance here, then its prediction that $Pr(D=1|R) > Pr(D=1)$ implies that it is likely that Fry's estimated probabilities are equal because $Pr(S\\cap D = 1|R) > Pr(S\\cap D = 1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "responsible-dominant",
   "metadata": {},
   "source": [
    "**(4)** The Justice Department should care about which (Fry's or the \"driving while Black\") is the better model. How might one go about testing one against the other?\n",
    "\n",
    "**Answer** \n",
    "\n",
    "The main conflict between these models is that \"driving while Black\" proposes that $P(D = 1 | R) > P(D = 1)$, whereas Fry's model requires $P(D = 1 | R)= P(D = 1)$ for his identification strategy. Therefore, testing these models against eachother only requires evidence of $P(D=1)$ vs. $P(D=1|R)$. For example, Horrace and Rohlin (2016) exploit the relative darkness of nightand hence deteriorated ability of police officers to identify the race of someone they may stop) to measure racial profiling, and find that the \"odds of a black driver being stopped (relative to nonblack drivers) increase 15% in daylight compared to darkness\" (Horrace and Rohlin, 2016).\n",
    "\n",
    "References:\n",
    "\n",
    "William C. Horrace, Shawn M. Rohlin; How Dark Is Dark? Bright Lights, Big City, Racial Profiling. The Review of Economics and Statistics 2016; 98 (2): 226–232. doi: https://doi.org/10.1162/REST_a_00543"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fancy-mainstream",
   "metadata": {},
   "source": [
    "# 5. Weighting of Linear IV Estimators\n",
    "Consider the Linear IV model\n",
    "\n",
    "$y = X\\beta + u \\qquad\\mathbb{E}(Z'u) = 0$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-proposal",
   "metadata": {},
   "source": [
    "**(1)** Exploiting the moment condition, under what conditions does the estimator $b_{IV}$ satisfying $Z'y = (Z'X)b_{IV}$ consistently estimate $\\beta$?\n",
    "\n",
    "**Answer**\n",
    "Hansen (2021) outlines the conditions under which $IV$ is consistent. The moment conditions already provide the assumption that $\\mathbb{E}(Z'u) = 0$. Additionally supposing the data is \n",
    "iid, and that each of $y, X, Z$ have finite variance, then we only need:\n",
    "\n",
    "1. $\\mathbb{E}(Z'Z)$ positive definite, and\n",
    "2. $\\mathbb{E}(Z'X)$ full column rank (instrument relevance)\n",
    "\n",
    "for consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consecutive-helena",
   "metadata": {},
   "source": [
    "**(2)**  Suppose that $Z$ has $l$ columns. Construct a symmetric, $l\\times l$ full rank matrix $W$, and a corresponding estimator $b_W$ satisfying $WZ'y = W(Z'X)b_W$. Under what conditions will this estimator consistently estimate $\\beta$?\n",
    "\n",
    "**Answer** This estimator will consistently estimate $\\beta$ for any full rank matrix $W$ such that $\\mathbb{E}(WZ'X)$ is full column rank, plus the regularity conditions about the data being iid and $y, X, Z$ having finite variance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "missing-closing",
   "metadata": {},
   "source": [
    "**(3)** Describe the GMM criterion function that bW minimizes.\n",
    "\n",
    "**Answer**\n",
    "\n",
    "$b_W = argmin \\; $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "affecting-virus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.58409253,  0.06178896, -0.54990327],\n",
       "       [ 0.06178896,  0.5944447 , -0.00868079],\n",
       "       [-0.54990327, -0.00868079,  0.20041244]])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import distributions as iid\n",
    "import numpy as np\n",
    "\n",
    "x = iid.norm(scale=1).rvs(size = (1000, 1))\n",
    "z = iid.norm(scale=np.sqrt(4)).rvs(size = (1000, 1))\n",
    "z2 = iid.norm(scale=np.sqrt(8)).rvs(size = (1000, 1))\n",
    "x2 = x**2\n",
    "x3 = x**3\n",
    "\n",
    "X = np.hstack((x, x2, x3))\n",
    "\n",
    "Z = np.hstack((x, z))\n",
    "xdem = X - X.mean(axis = 0)\n",
    "np.linalg.inv(xdem.T@xdem/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "ahead-stranger",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.93306526, -0.0596372 ,  2.55761536],\n",
       "       [-0.0596372 ,  1.68880374, -0.09048611],\n",
       "       [ 2.55761536, -0.09048611, 12.00851862]])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.cov(X, rowvar = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "intellectual-footwear",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.70710678, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.5       , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.40824829, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.02238307, 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.02237187,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.02236068]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this gives Ω to be:\n",
    "from scipy.linalg import inv, sqrtm, pinv\n",
    "\n",
    "N = 1000\n",
    "x = 1\n",
    "xs = []\n",
    "for i in range(1000):\n",
    "    xs.append(x0*2*(i+1))\n",
    "Omega = 2*np.eye(N) # 0.2 from above definition of u\n",
    "\n",
    "\n",
    "Omega_inv = inv(np.diag(xs))\n",
    "\n",
    "# get square root of omega_inv\n",
    "lamb, V = np.linalg.eig(Omega_inv)\n",
    "D_sqrt = np.diag(lamb)**(1/2)\n",
    "Omega_sqrt = V@D_sqrt@inv(V)\n",
    "Omega_sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "twelve-realtor",
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = iid.norm(scale=np.sqrt(2)).rvs(size = (10, 1))\n",
    "\n",
    "u = iid.norm(scale=1).rvs(size = (10, 1))\n",
    "rho = 0.01\n",
    "x1 = [0.2]\n",
    "for i in range(10):\n",
    "    x1.append((rho*x0[i] + u[i])[0])\n",
    "    \n",
    "u = np.array(x)[:10].reshape(10,1)\n",
    "\n",
    "u2 = [0.1]\n",
    "\n",
    "for i in range(10):\n",
    "    u2.append((10+rho*x1[i] + u[i])[0])\n",
    "    \n",
    "u2 = np.array(u2)[:10].reshape(10,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "premier-association",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.01974014,  0.19977341, -0.10027989],\n",
       "       [ 0.19977341,  0.2228437 , -0.11519573],\n",
       "       [-0.10027989, -0.11519573,  0.06268941]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U = np.hstack((x0, u, u2))\n",
    "\n",
    "np.linalg.inv(U.T@U/10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
